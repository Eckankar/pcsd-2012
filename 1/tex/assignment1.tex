\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\title{Principles of Computer System Design\\Assignment 1}
\author{Jacob Wejendorp\\Sebastian Paaske Tørholm\\Kasper Fabæch Brandt}

\begin{document}
\maketitle
\section{Exercises}
\subsection{Question 1, Fundamental abstractions}

\subsection{Question 2, Hardware trends}

\begin{tabular}{|r|c|c|c|}
    \hline
                    & HDD & SSD & RAM\\\hline
    Access time     & & & \\\hline
    Capacity (GB)   & 1000-2000 & 128-256 & 1-4 \\\hline
    Cost (kr/GB)    & 0.35 & 4.5-5.5 & 32 \\\hline
    Reliability     & & & \\\hline
    Power consumption & & & \\\hline

\end{tabular}
Capacity and cost pr capacity data retrieved from EDBpriser.dk, Nov 29 2012.
Ram - 1 block at a time, no sets.

Reliability - read errors? Mean time to failure?


\subsection{Question 3, Performance}
\subsubsection{How does concurrency influence latency?}

The latency will be lower, if the overhead of scheduling, work partitioning and
other concurrency related overhead is less than the gain from parallelizing the
computation and vice versa. This will in practice favor larger computations and
datasets, while influencing smaller jobs negatively.

\subsubsection{What is the difference between dallying and batching?}

Batching is the act of performing several related actions simultaneously.
Dallying is the act of postponing an action, in order to later potentially batch
the action together with another related action, or maybe avoid doing it at all.

As an example, we can look at memory mapped files, where data is read from the
disk in batches (pages).
Write requests are dallied by only writing changes to ram, postponing disk
writes until strictly necessary, potentially batching several changes to a page
into one disk write.

\subsubsection{Is caching an example of fast path optimization?}

Caching exploits the fact that more frequently accessed items are more likely
to be in the cache. By this virtue, caching will often improve the common case
and is therefore an example of fast path optimization.

\subsection{Question 4, Experimental design}
\subsubsection{How to test bandwidth in case of only cache hits}
\begin{itemize}
    \item Allocate a small piece of memory. (For instance the size of a page)
    \item Initialize the memory to all 0s. It should now be cached.
    \item Start timing.
    \item Iterate through each byte in the memory n times, incrementing each byte
          and saving it back. This should all hit cache.
    \item Stop timing.
\end{itemize}

Simple math can now be performed based on the size of the block of memory and
the time taken, in order to figure out the bandwidth achieved.

\subsubsection{How to test bandwidth in case of only cache misses}
\begin{itemize}
    \item Allocate a large piece of memory, at least twice the size of the cache.
    \item Initialize the memory to all 0s. A reasonable cache would now have the last part of the memory allocated cached. 
    \item Start timing.
    \item Iterate through the memory n times, accessing only the first byte
          of each page. Since memory is cached in pages, each access should be
          unaffected by the one before it. The size of the allocated memory should
          ensure that by the time a page is accessed, it isn't cached.
    \item Stop timing.
\end{itemize}

The same math can be performed as in the first experiment to determine the
bandwidth achieved.

\end{document}

